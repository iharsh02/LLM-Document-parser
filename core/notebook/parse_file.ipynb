{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "* load the env from .env file\n",
    "* setup the connection with rabbitMQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "\n",
    "\n",
    "RABBITMQ_URL = os.environ.get(\"RABBITMQ_URL\")\n",
    "LANGSMITH_TRACING = os.environ.get(\"LANGSMITH_TRACING\")\n",
    "LANGSMITH_API_KEY = os.environ.get(\"LANGSMITH_API_KEY\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Receiving the msg from the queue\n",
    "\n",
    "Open a connection to a rabbitMQ parse-file queue and get the results , process the result and store them into Reddis for temp storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Connected to RabbitMQ \n"
     ]
    }
   ],
   "source": [
    "import os , sys , json , requests\n",
    "import pika\n",
    "from io import BytesIO\n",
    "from PyPDF2 import  PdfReader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "RABBITMQ_URL =os.getenv('RABBITMQ_URL')\n",
    "QUEUE = \"parse-files\"\n",
    "\n",
    "# Establish connection to RabbitMQ\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "channel.queue_declare(queue=QUEUE, durable=True)\n",
    "print(\"[*] Connected to RabbitMQ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The idea here is to store numneric vecotrs that are associated with the text given a query we can embed it as a vector of the same dimension and user vector similarity metrics to identify related text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import  OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Documents \n",
    "\n",
    "Get the JOB from the rabbitMQ \n",
    "```\n",
    "{\n",
    "  \"jobId\": \"b23557ab-e8d0-4f04-96e6-dbfa42aec980\",\n",
    "  \"files\": [\n",
    "    {\n",
    "      \"url\": \"https://res.cloudinary.com/dzfbs8rep/image/upload/v1757932874/claims/eavjy7iks164ptd6ndgq.pdf\",\n",
    "      \"public_id\": \"claims/eavjy7iks164ptd6ndgq\",\n",
    "      \"bytes\": 106362,\n",
    "      \"format\": \"pdf\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "fetch the url downlaod the pdf parse it and store it into a in memory vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job(body: bytes):\n",
    "    \"\"\"Decode JSON, download multiple PDFs, extract text, store with metadata\"\"\"\n",
    "    job = json.loads(body.decode(\"utf-8\"))\n",
    "    job_id = job[\"jobId\"]\n",
    "\n",
    "    print(f\"\\n[*] Job Received: {job_id}\")\n",
    "\n",
    "    for idx, f in enumerate(job[\"files\"], start=1):\n",
    "        url = f[\"url\"]\n",
    "        fmt = f.get(\"format\", \"\").lower()\n",
    "        name = f.get(\"public_id\", f\"file{idx}\")\n",
    "\n",
    "        print(f\" [*] downloading {fmt} from {url}\")\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        if fmt == \"pdf\":\n",
    "            reader = PdfReader(BytesIO(resp.content))\n",
    "\n",
    "            # Extract metadata\n",
    "            meta = reader.metadata or {}\n",
    "            num_pages = len(reader.pages)\n",
    "\n",
    "            # Turn into a serializable dict\n",
    "            meta_data = {\n",
    "                \"author\": meta.get(\"/Author\"),\n",
    "                \"title\": meta.get(\"/Title\"),\n",
    "                \"subject\": meta.get(\"/Subject\"),\n",
    "                \"creator\": meta.get(\"/Creator\"),\n",
    "                \"producer\": meta.get(\"/Producer\"),\n",
    "                \"created\": str(meta.get(\"/CreationDate\")),\n",
    "                \"modified\": str(meta.get(\"/ModDate\")),\n",
    "                \"pages\": num_pages,\n",
    "                \"source_url\": url,\n",
    "                \"public_id\": name,\n",
    "            }\n",
    "\n",
    "            # Extract text\n",
    "            text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "            print(f\" [*] {name}: extracted {len(text)} characters, {num_pages} pages\")\n",
    "        else:\n",
    "            meta_data = {\"source_url\": url, \"public_id\": name, \"format\": fmt}\n",
    "            text = f\"[Unsupported format: {fmt}]\"\n",
    "            print(f\" [*] Unsupported format: {fmt}\")\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=800,\n",
    "                                                  chunk_overlap=200)\n",
    "        chunks = splitter.split_text(text)\n",
    "        document_ids = vector_store.add_texts(texts=chunks)\n",
    "        print(f\"{len(chunks)}\") \n",
    "\n",
    "        print(document_ids[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback function - listen to the queue process the job and stop consuming after getting 1 job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(ch, method, properties, body):\n",
    "    \"\"\"Callback for messages from RabbitMQ\"\"\"\n",
    "    try:\n",
    "        process_job(body)\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "        ch.stop_consuming()\n",
    "    except Exception as e:\n",
    "        print(\" [*] Error processing:\", e)\n",
    "        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*] Job Received: 11ac6f16-1624-40a9-81ba-38c4edca1352\n",
      " [*] downloading pdf from https://res.cloudinary.com/dzfbs8rep/image/upload/v1757937705/claims/z3dm6odsjfskmiait07j.pdf\n",
      " [*] claims/z3dm6odsjfskmiait07j: extracted 1186 characters, 1 pages\n",
      "2\n",
      "56a39c90-21ec-464a-872c-1d002b44d74f\n",
      " [*] Consumer stopped. Closing connection.\n"
     ]
    }
   ],
   "source": [
    "channel.basic_qos(prefetch_count=1)\n",
    "channel.basic_consume(queue=QUEUE, on_message_callback=callback)\n",
    "\n",
    "try:\n",
    "    channel.start_consuming()\n",
    "    print(\" [*] Consumer stopped. Closing connection.\")\n",
    "    connection.close()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user, closing connection...\")\n",
    "    channel.stop_consuming()\n",
    "    connection.close()\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test : Checking the seimilarity sementic search is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Health Insurance Policy Document\n",
      "Policy Holder Information:\n",
      "Name: John Doe\n",
      "Age: 46\n",
      "Gender: Male\n",
      "Address: 123, Green Park, Pune, Maharashtra, India\n",
      "Policy Number: HIP-2025-000123\n",
      "Policy Start Date: 2025-01-01\n",
      "Policy Duration: 1 year\n",
      "Coverage Clauses:\n",
      "Clause 1.1: The policy covers medical expenses related to hospitalization due to accidents, illnesses, and surgeries.\n",
      "Clause 1.2: Outpatient consultations are covered up to â– 10,000 annually.\n",
      "Clause 2.3: Coverage applicable for policyholders aged between 18 and 65 years.\n",
      "Clause 3.2: Knee surgery is covered if the policy has been active for at least 90 days.\n",
      "Clause 3.5: Cardiac procedures are covered if recommended by a certified specialist.\n",
      "Clause 4.1: Policies must be valid and active in the location where the medical procedure is performed.'\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"Knee surgery\"\n",
    ")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Implementation using Langchain\n",
    "\n",
    "Using langChain RAG implementation to get back the JSON result we want\n",
    "{ decision, amount, justification, clauses_used }\n",
    "\n",
    "- Index parsed text into vector DB.\n",
    "\n",
    "- At query time: embed user query, retrieve most relevant clauses.\n",
    "\n",
    "- Pass into LLM with system+user prompts.\n",
    "\n",
    "- Get a structured JSON decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert Document Analysis AI specialized in interpreting insurance policy documents. Your job is to process an input query and output a structured JSON object with a clear decision, based on the policy clauses.\n",
    "\n",
    "The expected JSON output format is:\n",
    "\n",
    "{{\n",
    "  \"name\": \"Full Name\",\n",
    "  \"age\" : \"age\"\n",
    "  \"decision\": \"approved\" | \"rejected\",\n",
    "  \"amount\": 50000,\n",
    "  \"justification\": \"Detailed explanation with clause numbers and reasoning.\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "1. Parse the input query to extract key details: age, procedure, location, policy duration.\n",
    "2. Perform semantic search over the provided policy document to retrieve relevant clauses.\n",
    "3. Evaluate the clauses and decide whether the claim is approved or rejected.\n",
    "4. If approved, set the correct payout amount.\n",
    "5. Provide a detailed justification mentioning clause numbers and reasoning.\n",
    "6. Output only the valid JSON object (no extra text).\n",
    "\n",
    "Context (policy document):\n",
    "{context}\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)  \n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"age\": \"46\",\n",
      "  \"decision\": \"approved\",\n",
      "  \"amount\": 50000,\n",
      "  \"justification\": \"The claim for knee surgery is approved based on Clause 1.1, which covers medical expenses related to surgeries. The policyholder is within the age limit specified in Clause 2.3 (aged between 18 and 65 years). Although the policy has only been active for 3 months, Clause 3.2 states that knee surgery is covered if the policy has been active for at least 90 days, which is satisfied. The location of the procedure (Pune) is valid as per Clause 4.1. The maximum payout per approved claim is set at 50,000 as per Clause 5.1.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\n",
    "    \"question\": \"46M, knee surgery, Pune, activated for 3-month policy\",\n",
    "    \"context\": document_ids\n",
    "})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
